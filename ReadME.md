# 📊 FinQA LLM App — Financial Question Answering with Fine-Tuned Models

This project demonstrates a Streamlit-based application for answering financial questions using fine-tuned LLMs.  It supports uploading real-world financial documents and generates accurate, model-driven answers.

---

## 🔍 What This App Does

- 📁 Upload financial `.json` documents (from `test_documents/`)
- ❓ Ask natural-language financial questions
- 🤖 Choose a model: Mistral, GPT-Neo, or TinyLLaMA
- 💬 Receive context-aware answers generated by the selected model
- 📊 Compare metrics and visualize performance across models

---

## 🧠 Models Used

| Model      | Size     | Fine-Tuned | Access |
|------------|----------|------------|--------|
| Mistral    | 7B       | ✅         | [Google Drive](https://drive.google.com/drive/folders/1cuxBjfQcvEwZl8PrX4fbsFucH7PJMwri?usp=drive_link) |
| GPT-Neo    | 1.3B     | ✅         | [Same link](https://drive.google.com/drive/folders/1cuxBjfQcvEwZl8PrX4fbsFucH7PJMwri?usp=drive_link) |
| TinyLLaMA  | 1.1B     | ✅         | [Same link](https://drive.google.com/drive/folders/1cuxBjfQcvEwZl8PrX4fbsFucH7PJMwri?usp=drive_link) |

> These models were trained on the [ConvFinQA dataset](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/ConvFinQA) and tested on both seen and unseen financial questions.

---

## 🚀 How to Run the App

### 1. Clone the repository & install dependencies

git clone https://github.com/PBUKE/finqa-demo

cd finqa-demo

pip install -r requirements.txt

### 2. Launch the Streamlit UI

streamlit run app.py

### 3. 📁 Project Structure

📁 FinQA LLM App
├── app.py                         # 🚀 Main Streamlit interface
├── test_documents/               # 📂 Uploadable external financial documents (.json)
├── data/                         # 📂 ConvFinQA dataset (train.json, dev.json, etc.)
├── models/                       # 🧠 Model runners and wrappers
│   ├── runner_mistral.py         # Mistral inference script
│   ├── runner_gptneo.py          # GPT-Neo inference script
│   └── runner_tinyllama.py       # TinyLLaMA inference script
├── utils/                        # 🔧 Utilities for parsing, formatting, and evaluation
│   ├── data_utils.py             # Extract tables, context, and questions
│   ├── formatters.py             # String and number cleaning utilities
│   └── eval_metrics.py           # Evaluation metric helpers
├── notebooks/                    # 📓 Preprocessing, training, and evaluation notebooks
│   ├── data_preparation.ipynb    # Convert train.json to prompt-response
│   ├── model_evaluation.ipynb    # Visualize accuracy, F1, SMAPE, etc.
│   └── finetuning_colab          # LoRA/QLoRA training notebook for Colab
├── results/                      # 📈 Model predictions and logs
│   ├── dev_eval_mistral.json     # Output predictions for dev set (Mistral)
│   ├── test_eval_gptneo.json     # Output predictions for test set (GPT-Neo)
│   └── accuracy_comparison.png   # Evaluation chart comparing models          
├── requirements.txt              # 📦 Python dependencies
└── README.md                     # 📘 Project documentation

<img width="688" alt="image" src="https://github.com/user-attachments/assets/bb4a02d8-2146-4800-8d46-8addaab8d4ef" />


### 4. 🧩 Future Improvements
🔁 Integrate RAG (Retrieval-Augmented Generation) for better generalization

🧽 Normalize model outputs (%, $, rounding) during evaluation

📊 Add visual dashboards for answer evaluation

📦 Bundle as a pip package or deploy to Hugging Face Spaces

👩‍💻 Author
Pinar Buke
AI & Data Consultant

