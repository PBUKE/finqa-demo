# ğŸ“Š FinQA LLM App â€” Financial Question Answering with Fine-Tuned Models

This project demonstrates a Streamlit-based application for answering financial questions using fine-tuned LLMs.  It supports uploading real-world financial documents and generates accurate, model-driven answers.

---

## ğŸ” What This App Does

- ğŸ“ Upload financial `.json` documents (from `test_documents/`)
- â“ Ask natural-language financial questions
- ğŸ¤– Choose a model: Mistral, GPT-Neo, or TinyLLaMA
- ğŸ’¬ Receive context-aware answers generated by the selected model
- ğŸ“Š Compare metrics and visualize performance across models

---

## ğŸ§  Models Used

| Model      | Size     | Fine-Tuned | Access |
|------------|----------|------------|--------|
| Mistral    | 7B       | âœ…         | [Google Drive](https://drive.google.com/drive/folders/1cuxBjfQcvEwZl8PrX4fbsFucH7PJMwri?usp=drive_link) |
| GPT-Neo    | 1.3B     | âœ…         | [Same link](https://drive.google.com/drive/folders/1cuxBjfQcvEwZl8PrX4fbsFucH7PJMwri?usp=drive_link) |
| TinyLLaMA  | 1.1B     | âœ…         | [Same link](https://drive.google.com/drive/folders/1cuxBjfQcvEwZl8PrX4fbsFucH7PJMwri?usp=drive_link) |

> These models were trained on the [ConvFinQA dataset](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/ConvFinQA) and tested on both seen and unseen financial questions.

---

## ğŸš€ How to Run the App

### 1. Clone the repository & install dependencies

git clone https://github.com/PBUKE/finqa-demo

cd finqa-demo

pip install -r requirements.txt

### 2. Launch the Streamlit UI

streamlit run app.py

### 3. ğŸ“ Project Structure

ğŸ“ FinQA LLM App
â”œâ”€â”€ app.py                         # ğŸš€ Main Streamlit interface
â”œâ”€â”€ test_documents/               # ğŸ“‚ Uploadable external financial documents (.json)
â”œâ”€â”€ data/                         # ğŸ“‚ ConvFinQA dataset (train.json, dev.json, etc.)
â”œâ”€â”€ models/                       # ğŸ§  Model runners and wrappers
â”‚   â”œâ”€â”€ runner_mistral.py         # Mistral inference script
â”‚   â”œâ”€â”€ runner_gptneo.py          # GPT-Neo inference script
â”‚   â””â”€â”€ runner_tinyllama.py       # TinyLLaMA inference script
â”œâ”€â”€ utils/                        # ğŸ”§ Utilities for parsing, formatting, and evaluation
â”‚   â”œâ”€â”€ data_utils.py             # Extract tables, context, and questions
â”‚   â”œâ”€â”€ formatters.py             # String and number cleaning utilities
â”‚   â””â”€â”€ eval_metrics.py           # Evaluation metric helpers
â”œâ”€â”€ notebooks/                    # ğŸ““ Preprocessing, training, and evaluation notebooks
â”‚   â”œâ”€â”€ data_preparation.ipynb    # Convert train.json to prompt-response
â”‚   â”œâ”€â”€ model_evaluation.ipynb    # Visualize accuracy, F1, SMAPE, etc.
â”‚   â””â”€â”€ finetuning_colab          # LoRA/QLoRA training notebook for Colab
â”œâ”€â”€ results/                      # ğŸ“ˆ Model predictions and logs
â”‚   â”œâ”€â”€ dev_eval_mistral.json     # Output predictions for dev set (Mistral)
â”‚   â”œâ”€â”€ test_eval_gptneo.json     # Output predictions for test set (GPT-Neo)
â”‚   â””â”€â”€ accuracy_comparison.png   # Evaluation chart comparing models          
â”œâ”€â”€ requirements.txt              # ğŸ“¦ Python dependencies
â””â”€â”€ README.md                     # ğŸ“˜ Project documentation

<img width="688" alt="image" src="https://github.com/user-attachments/assets/bb4a02d8-2146-4800-8d46-8addaab8d4ef" />


### 4. ğŸ§© Future Improvements
ğŸ” Integrate RAG (Retrieval-Augmented Generation) for better generalization

ğŸ§½ Normalize model outputs (%, $, rounding) during evaluation

ğŸ“Š Add visual dashboards for answer evaluation

ğŸ“¦ Bundle as a pip package or deploy to Hugging Face Spaces

ğŸ‘©â€ğŸ’» Author
Pinar Buke
AI & Data Consultant

